services:

  ollama-server:  # Assumes Dockerfile for your app is in the current directory
    build: 
      context: ./Server
    ports:
      - "11434:11434"  # Expose Ollama on port 11434 to the host
    networks: # network containers are on
      - app-network
    deploy:
      resources: # if resources are not allocated server starves cpu causing application to fail
        limits:
          cpus: "4.0"    # Allocate 4 cores
          memory: "6g"   # Allocate 8GB RAM
        reservations:
          cpus: "2.0"    # Ensure 2 cores are always reserved
          memory: "4g"   # Ensure 4GB RAM is always reserved

  ollama-app:
    build:
      context: ./App  # Assumes Dockerfile for your app is in the current directory
    depends_on: # waits for server to start
      - ollama-server
    environment:  # Connects to Ollama using the internal network
      - OLLAMA_SERVER_URL=${OLLAMA_SERVER_URL}
    networks: # network containers are on
      - app-network
    stdin_open: true   # Keeps stdin open
    tty: true          # Enables tty for interactive terminal

networks:
  app-network:
    driver: bridge