"""
Authors: Antonio Sousa Jr(Team Lead), Matthew Greeson, Goncalo Felix, Antonio Morais, Dylan Ricci, Ryan Medeiros
Affiliation: University of Massachusetts Dartmouth
Course: CIS 498 & 499 (Senior Capstone Project)
Ownership: Rite-Solutions, Inc.
Client/Stakeholder: Brandon Carvalho
"""


from langchain_ollama import OllamaLLM
from langchain.schema import Document
from langchain_milvus import Milvus
from llm_package import collection_manger
from collections import defaultdict

from dotenv import load_dotenv
import os

# Get ENV variables
load_dotenv()
OLLAMA_SERVER_URL = os.getenv("OLLAMA_SERVER_URL")

# Initialize llm models you would like to use, Mix and match differnt models for best result
llm1 = OllamaLLM(model="llama3.1:latest", base_url=OLLAMA_SERVER_URL)
llm2 = OllamaLLM(model="llama3.1:latest", base_url=OLLAMA_SERVER_URL)


def retrieve_docs(query: str, vector_store: Milvus) -> list[Document]:
    # Function to retrieve relevant document chunks based on a user's query

    try:
        # Performing a broad search to allow for selection from chunks
        raw_results = vector_store.similarity_search(query=query, k=50)

        # Grouping the results by filetype
        type_buckets = defaultdict(list)
        for doc in raw_results:
            filetype = doc.metadata.get("filetype", "unknown")
            type_buckets[filetype].append(doc)

        # Collect balanced results from each filetype
        balanced_results = []
        for doc in type_buckets.values():
            balanced_results.extend(doc[:5])  # Take up to 5 from each type

        return balanced_results[:20]

    except Exception as e:
        print(f"[retrieve_docs] Failed during balanced retrieval: {e}")
        raise


def create_prompt(retrieved_docs: list[Document], query: str) -> str:
    # Function to create a detailed prompt with context for the LLM

    prompt = (
        "You are an assistant for question-answering tasks. Please only "
        "use the following pieces of retrieved context to answer the question. "
        "If you cannot find any context referring to the question, inform the "
        "user that you could not find any context related to that concept."
        "Use up to five sentences maximum and keep the answer concise but, if the "
        "user specifies how many sentences to use, only use that amount of sentences. "
    )

    Context_str = ""

    # Add the context from the retrieved documents to the prompt
    for i in retrieved_docs:
        Context_str += i.page_content + "\n\n"

    # Contruct final prompt
    final_prompt = (
        f"Prompt: {prompt}\n\n"
        f"Question: {query}\n\n"
        f"Context: {Context_str}\n\n"
        f"Answer: "
    )

    # Return the final prompt
    return final_prompt


def refine_prompt(retrieved_docs: list[Document], query: str, llm1_context: str) -> str:
    # Alternative prompt creation function with an additional LLM context

    prompt = (
        "You are an assistant for question-answering tasks. Please only use the "
        "following pieces of retrieved context to answer the question. If you cannot "
        "find any context referring to the question, inform the user that you could "
        "not find any context related to that concept. Use up to five sentences maximum "
        "and keep the answer concise but, if the user specifies how many sentences to "
        "use, only use that amount of sentences. "
    )

    Context_str = ""

    # Add the context from the retrieved documents to the prompt
    for i in retrieved_docs:
        Context_str += i.page_content + "\n\n"

    # Contruct final prompt with context from llm 1
    refined_prompt = (
        f"Prompt: {prompt}\n\n"
        f"Question: {query}\n\n"
        f"Context: {Context_str}\n\n"
        "This is an answer generated by another LLM, try to use this to make a "
        f"better and more insightful response: {llm1_context}\n\n"
        f"Answer: "
    )

    # Return the refined prompt
    return refined_prompt


def get_llm_response(llm: OllamaLLM, prompt: str) -> str:
    # Function to query the LLM with the generated prompt and return the answer

    response = llm.invoke(prompt)

    # Returns the llm response
    return response


def full_rag_response(query: str) -> str:
    # Combines previous methods to create full rag response

    try:
        # Retrive docs
        retrieved_docs = retrieve_docs(
            query=query, vector_store=collection_manger.get_milvus_connection()
        )

        # Create prompt
        prompt = create_prompt(query=query, retrieved_docs=retrieved_docs)

        # Gets response
        response = get_llm_response(llm=llm1, prompt=prompt)
        print(f"Reponse: {response}")

        # Refines prompt
        refined_prompt = refine_prompt(
            query=query, retrieved_docs=retrieved_docs, llm1_context=response)

        # Gets refined response
        refined_response = get_llm_response(llm=llm2, prompt=refined_prompt)
        print(f"Refined Reponse: {refined_response}")

        # Returns refined response
        return refined_response

    except Exception as e:
        print(f"[full_rag_response] Failed to generate rag response: {e}")
        raise
