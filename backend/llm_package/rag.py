"""
File: llm.py
Authors: Antonio Sousa Jr(Team Lead), Matthew Greeson, Goncalo Felix, Antonio Morais, Dylan Ricci, Ryan Medeiros
Affiliation: University of Massachusetts Dartmouth
Course: CIS 498 & 499 (Senior Capstone Project)
Ownership: Rite-Solutions, Inc.
Client/Stakeholder: Brandon Carvhalo
Date: 2025-4-25
Project Description: This code utilizes Ollama as our LLM and a Retrieval-Augmented Generation (RAG) approach to take documents from a specifc directory, load them and then
             split the documents into texts and store it into a local database using Milvus Lite. Once stored the user then asks a questions which retrieves the most
             relevant documents and the LLM generates a response as best as it can.

"""

# For using the Ollama language model
from langchain_ollama import OllamaLLM
from langchain.schema import Document  # Document schema class
from langchain_milvus import Milvus  # For managing vector databases

class Rag:

    previous_questions = []
    previous_answers = []

    # Function to retrieve relevant documents based on a user's query
    def retrieve_docs(
        self, query: str, vector_store: Milvus
    ) -> list[Document]:
        
        search_results = vector_store.similarity_search(
            query=query, k=3
            )
        
        return search_results

    # Function to create a detailed prompt with context for the LLM
    def create_prompt(self, retrieved_docs: list[Document], query: str) -> str:
        prompt = (
            "You are an assistant for question-answering tasks. Please only use the following pieces of retrieved context to answer the question. If you cannot find any context referring to the question, inform the user that you could not find any context related to that concept."
            "Please do not use previously stored knowledge to answer the questions. Use up to five sentences maximum and keep the answer concise but, if the user specifies how many sentences to use, only use that amount of sentences. "
            "Use the previous questions and answers to be conversational if need be.  \n"
        )

        # Format the question, context, and previous interactions
        Question = "Question: " + query + "\n"
        Context_str = "Context: \n\n"

        # Add the context from the retrieved documents to the prompt
        for i in retrieved_docs:
            Context_str += i.page_content + "\n\n"

        Answer = "Answer: "

        previous_questions_str = "previous questions: \n\n"
        for i in range(len(self.previous_questions)):
            previous_questions_str += f"Q{i+1}: {self.previous_questions[i]}\n\n"

        previous_answers_str = "previous answers: \n\n"
        for i in range(len(self.previous_answers)):
            previous_answers_str += f"A{i+1}: {self.previous_answers[i]}\n\n"

        final_prompt = (
            prompt + Question + Context_str +
            previous_questions_str + previous_answers_str + Answer
        )
        return final_prompt

    # Alternative prompt creation function with an additional LLM context
    def refine_prompt(
        self,
        retrieved_docs: list[Document],
        llm1_context: str,
        question: str,
    ) -> str:

        prompt = (
            "You are an assistant for question-answering tasks. Please only use the following pieces of retrieved context to answer the question. If you cannot find any context referring to the question, inform the user that you could not find any context related to that concept."
            "Please do not use previously stored knowledge to answer the questions. Use up to five sentences maximum and keep the answer concise but, if the user specifies how many sentences to use, only use that amount of sentences. "
            "Use the previous questions and answers to be conversational if need be.  \n"
        )
        Question = "Question: " + question + "\n"
        Context_str = "Context: \n\n"
        Answer = "Answer: "

        # Add the context from the retrieved documents to the prompt
        for i in retrieved_docs:
            Context_str += i.page_content + "\n\n"

        # Add the additional LLM context
        llm1_context = (
            "This is an answer generated by another LLM, try to use this to make a better and more "
            "insightful response: " + llm1_context + "\n"
        )

        Answer = "Answer: "
        final_prompt = (
            prompt
            + Question
            + Context_str
            + llm1_context
            + Answer
        )

        return final_prompt

    # Function to query the LLM with the generated prompt and return the answer
    def get_llm_response(self, llm: OllamaLLM, prompt: str) -> str:
        response = llm.invoke(prompt)  # Get the answer from the LLM
        return response
