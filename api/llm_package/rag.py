"""
File: llm.py
Authors: Antonio Sousa Jr(Team Lead), Matthew Greeson, Goncalo Felix, Antonio Morais, Dylan Ricci, Ryan Medeiros
Affiliation: University of Massachusetts Dartmouth
Course: CIS 498 & 499 (Senior Capstone Project)
Ownership: Rite-Solutions, Inc.
Client/Stakeholder: Brandon Carvhalo
Date: 2025-4-25
Project Description: This code utilizes Ollama as our LLM and a Retrieval-Augmented Generation (RAG) approach to take documents from a specifc directory, load them and then
             split the documents into texts and store it into a local database using Milvus Lite. Once stored the user then asks a questions which retrieves the most
             relevant documents and the LLM generates a response as best as it can.

"""

# For using the Ollama language model
from langchain_ollama import OllamaLLM, OllamaEmbeddings
from langchain.schema import Document  # Document schema class
from langchain_milvus import Milvus  # For managing vector databases
from langchain.vectorstores.base import VectorStoreRetriever

from ragas import EvaluationDataset, evaluate, SingleTurnSample
from ragas.metrics import (
    LLMContextPrecisionWithoutReference,
    Faithfulness,
    ResponseRelevancy,
)
from ragas.llms import LangchainLLMWrapper
from ragas.embeddings import LangchainEmbeddingsWrapper

class Rag:

    previous_questions = []
    previous_answers = []

    # Function to create a retriever for querying the vector store
    def create_retriever(self, vector_store: Milvus) -> VectorStoreRetriever:
        retriever = vector_store.as_retriever(
            search_type="similarity", search_kwargs={"k": 3}
        )
        return retriever

    # Function to retrieve relevant documents based on a user's query
    def retrieve_docs(
        self, retriever: VectorStoreRetriever, question: str
    ) -> list[Document]:
        # Retrieve top 15 relevant documents
        search_results = retriever.invoke(question)
        return search_results

    # Function to prompt the user for a question
    def get_query(self) -> str:
        # Prompt for user input
        query = input(f"What would you like to ask? \n")
        return query

    # Function to create a detailed prompt with context for the LLM
    def create_prompt(self, retrieved_docs: list[Document], query: str) -> str:
        prompt = (
            "You are an assistant for question-answering tasks. Use the following pieces "
            "of retrieved context to answer the question. Use five sentences maximum and keep the answer "
            "concise. Use the previous questions and answers to be conversational if need be.  \n"
        )

        # Format the question, context, and previous interactions
        Question = "Question: " + query + "\n"
        Context_str = "Context: \n\n"

        # Add the context from the retrieved documents to the prompt
        for i in retrieved_docs:
            Context_str += i.page_content + "\n\n"

        Answer = "Answer: "

        previous_questions_str = "previous questions: \n\n"
        for i in range(len(self.previous_questions)):
            previous_questions_str += f"Q{i+1}: {self.previous_questions[i]}\n\n"

        previous_answers_str = "previous answers: \n\n"
        for i in range(len(self.previous_answers)):
            previous_answers_str += f"A{i+1}: {self.previous_answers[i]}\n\n"

        final_prompt = (
            prompt + Question + Context_str +
            previous_questions_str + previous_answers_str + Answer
        )
        return final_prompt

    # Alternative prompt creation function with an additional LLM context
    def create_prompt2(
        self,
        retrieved_docs: list[Document],
        llm1_context: str,
        question: str,
        prevAnswer: str,
        prevQuestion: str,
    ) -> str:

        prompt = (
            "You are an assistant for question-answering tasks. Use the following pieces "
            "of retrieved context to answer the question. Use five sentences maximum and keep the answer "
            "concise. Use the previous question and answer if requested by the user. \n"
        )
        Question = "Question: " + question + "\n"
        Context_str = "Context: \n\n"

        # Add the context from the retrieved documents to the prompt
        for i in retrieved_docs:
            Context_str += i.page_content + "\n\n"

        # Add the additional LLM context
        llm1_context = (
            "This is an answer generated by another LLM, try to use this to make a better and more "
            "insightful response: " + llm1_context + "\n"
        )

        prevQuestion = (
            "If needed, this is the previous question asked of you: "
            + prevQuestion
            + "\n"
        )
        prevAnswer = (
            "If needed, this is the previous answer you provided the user: "
            + prevAnswer
            + "\n"
        )

        Answer = "Answer: "
        final_prompt = (
            prompt
            + Question
            + Context_str
            + llm1_context
            + prevQuestion
            + prevAnswer
            + Answer
        )

        return final_prompt

    # Function to query the LLM with the generated prompt and return the answer
    def get_llm_response(self, llm: OllamaLLM, prompt: str) -> str:
        response = llm.invoke(prompt)  # Get the answer from the LLM
        return response

    def eval_rag_response(
        self,
        question: str,
        retrived_docs: list[Document],
        answer: str,
        llm: OllamaLLM,
        llm_embeddings: OllamaEmbeddings,
    ) -> EvaluationDataset:

        evaluator_llm = LangchainLLMWrapper(llm)
        gen_embeddings = LangchainEmbeddingsWrapper(llm_embeddings)

        sample = SingleTurnSample(
            user_input=question,
            response=answer,
            retrieved_contexts=[doc.page_content for doc in retrived_docs],
        )

        dataset = EvaluationDataset(samples=[sample])

        # Context Precision is a metric that measures the proportion of relevant chunks in the retrieved_contexts
        # The Faithfulness metric measures how factually consistent a response is with the retrieved context
        # The ResponseRelevancy metric measures how relevant a response is to the user input

        evaluation = evaluate(
            dataset=dataset,
            metrics=[
                LLMContextPrecisionWithoutReference(),
                Faithfulness(),
                ResponseRelevancy(),
            ],
            llm=evaluator_llm,
            embeddings=gen_embeddings,
        )

        return evaluation

    # Function to save the generated answer to a file
    def save_answer_to_file(self, output_file2: str, answer: str):
        """
        Saves the answer to a file, splitting it into sentences and cleaning each one before writing.
        """
        with open(output_file2, "w") as file:
            sentences = answer.strip().split(".")  # Split answer into sentences
            for sentence in sentences:
                cleaned_sentence = sentence.strip()
                if cleaned_sentence:  # Ensure the sentence is not empty
                    # Write each sentence to the file
                    file.write(cleaned_sentence + ".\n")
